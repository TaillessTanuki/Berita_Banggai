# -*- coding: utf-8 -*-
"""Berita_Banggai-Preprocessing-Only.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e-1VVKyRqnA90z0b3YsEG8qOWp_VZVM8

# Libraries
"""

!pip install Sastrawi
!pip install clean-text
!pip install deep-translator

import pandas as pd
import numpy as np
from nltk.corpus import stopwords
import re
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer
from nltk.tokenize import word_tokenize
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary
from cleantext import clean
from collections import Counter

import sklearn

# !pip freeze > requirements.txt

"""# Preprocessing"""

# Delete Duplicates
def drop_duplicates_by_title(input_df, column= 'title'):
  modified_df = input_df.drop_duplicates(subset='title')
  return modified_df


# Delete Blank text (Nan) Rows
def delete_text_column_value_blank(input_df, column= 'text'):
  modified_df = input_df[input_df['text'] != '']


# Case Folding
def clean_lower(lwr):
    lwr = lwr.lower() # lowercase text
    return lwr


# Remove Numbers
def remove_numbers(text):
  text = re.sub('[0-9]+', '', text)
  return text


# Remove Punctuation
clean_spcl = re.compile('[/(){}\[\]\|@,;]')
clean_symbol = re.compile('[^0-9a-z]')
def clean_punct(text):
    text = clean_spcl.sub('', text)
    text = clean_symbol.sub(' ', text)
    return text


# Delete Whitespaces
def normalize_whitespace(text):
    corrected = str(text)
    corrected = re.sub(r"//t",r"\t", corrected)
    corrected = re.sub(r"( )\1+",r"\1", corrected)
    corrected = re.sub(r"(\n)\1+",r"\1", corrected)
    corrected = re.sub(r"(\r)\1+",r"\1", corrected)
    corrected = re.sub(r"(\t)\1+",r"\1", corrected)
    return corrected.strip(" ")

# Adding Stopwords
import pandas as pd
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

def custom_stopwords_removal(text):
    """
    Remove stopwords from the input text using custom stopword lists.

    Args:
    - text (str): Input text to be processed.

    Returns:
    - str: Processed text with stopwords removed.
    """
    # Define URLs of custom stopwords files
    url1 = 'https://raw.githubusercontent.com/TaillessTanuki/Berita_Banggai/main/ID-stopwords_banggai_addedwords.csv'
    url2 = 'https://raw.githubusercontent.com/TaillessTanuki/Berita_Banggai/main/ID-Stopwords(MasDevid).csv'

    # Read custom stopwords from CSV files
    df_stopword1 = pd.read_csv(url1, delimiter=";", encoding='cp1252')
    df_stopword2 = pd.read_csv(url2, delimiter=";", encoding='cp1252')

    # Extract custom stopwords from DataFrames
    custom_stopword1 = list(df_stopword1['stopword'])
    custom_stopword2 = list(df_stopword2['Stopwords'])

    # Initialize stopwords remover factory
    factory = StopWordRemoverFactory()

    # Combine default stopwords with custom stopwords
    stopwords = factory.get_stop_words() + custom_stopword1 + custom_stopword2

    # Remove stopwords from text
    filtered_text = ' '.join(word for word in text.split() if word.lower() not in stopwords)

    return filtered_text


# Stemming
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

def stem_text(text):
    """
    Perform stemming on the input text using Sastrawi Stemmer.

    Args:
    - text (str): Input text to be stemmed.

    Returns:
    - str: Stemmed text.
    """
    # Create stemmer
    stem_factory = StemmerFactory()
    stemmer = stem_factory.create_stemmer()

    # Perform stemming
    stemmed_text = stemmer.stem(text)

    return stemmed_text


# Tokenizing
def tokenize(text):
  tokens = nltk.tokenize.word_tokenize(text)
  return tokens


# 2nd Stopwords Removal
# Define a function to remove stopwords from a list of tokens
# Removing stopwords from tokens
def remove_stopwords(tokens):
    return [word for word in tokens if word not in stopwords]


def join_tokens_to_text(tokens):
    """
    Join tokens into a single text string.

    Args:
    - tokens (list): List of tokens to be joined.

    Returns:
    - str: Text string with tokens joined.
    """
    return ' '.join(tokens)