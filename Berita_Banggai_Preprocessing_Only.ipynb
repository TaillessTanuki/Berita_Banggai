{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj172v-Yd1-_"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjWHRVR5ahdx",
        "outputId": "9e08c2a6-70a1-4ba4-9f84-b6a14aa64a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: clean-text in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from clean-text) (1.7.0)\n",
            "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from clean-text) (6.2.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.13)\n",
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install Sastrawi\n",
        "!pip install clean-text\n",
        "!pip install deep-translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nsz9j1DEJXD",
        "outputId": "4b99ee0f-1541-4ca9-a1ca-f2dc1dcd1bf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
        "from cleantext import clean\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn"
      ],
      "metadata": {
        "id": "fSui9eOPzRQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKKzjVp3T644"
      },
      "outputs": [],
      "source": [
        "# !pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input\n",
        "text_url = 'https://raw.githubusercontent.com/TaillessTanuki/Berita_Banggai/main/input_text.csv'\n",
        "\n",
        "# Read CSV file with 'latin-1' encoding\n",
        "input_df = pd.read_csv(text_url, encoding='utf-8')\n",
        "\n",
        "# Convert all columns to string\n",
        "input_df = input_df.astype(str)"
      ],
      "metadata": {
        "id": "ZmVoxpgYjK49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_df.head()"
      ],
      "metadata": {
        "id": "SEWU_WUDt98k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "ypMTFDakovXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete Duplicates\n",
        "input_df.duplicated(subset='title').sum()\n",
        "input_df = input_df.drop_duplicates(subset='title')\n",
        "\n",
        "# Delete Blank text (Nan) Rows\n",
        "input_df = input_df[input_df['text'] != '']\n",
        "\n",
        "# Case Folding\n",
        "def clean_lower(lwr):\n",
        "    lwr = lwr.lower() # lowercase text\n",
        "    return lwr\n",
        "\n",
        "input_df['lwr'] = input_df['text'].apply(clean_lower)\n",
        "input_df['lwr']\n",
        "\n",
        "\n",
        "# Remove Numbers\n",
        "def remove_numbers(text):\n",
        "  text = re.sub('[0-9]+', '', text)\n",
        "  return text\n",
        "\n",
        "input_df['clean_number'] = input_df['lwr'].apply(remove_numbers)\n",
        "input_df['clean_number']\n",
        "\n",
        "\n",
        "# Remove Punctuation\n",
        "clean_spcl = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "clean_symbol = re.compile('[^0-9a-z]')\n",
        "def clean_punct(text):\n",
        "    text = clean_spcl.sub('', text)\n",
        "    text = clean_symbol.sub(' ', text)\n",
        "    return text\n",
        "\n",
        "input_df['clean_punct'] = input_df['clean_number'].apply(clean_punct)\n",
        "input_df['clean_punct']\n",
        "\n",
        "\n",
        "# Delete Whitespaces\n",
        "def normalize_whitespace(text):\n",
        "    corrected = str(text)\n",
        "    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n",
        "    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n",
        "    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n",
        "    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n",
        "    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n",
        "    return corrected.strip(\" \")\n",
        "\n",
        "input_df['clean_double_ws'] = input_df['clean_punct'].apply(normalize_whitespace)\n",
        "input_df['clean_double_ws']\n"
      ],
      "metadata": {
        "id": "VbX305x1oywh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_df)"
      ],
      "metadata": {
        "id": "SWMa0q52qfy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_df.head()"
      ],
      "metadata": {
        "id": "3WGgr4UyqaNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Stopwords\n",
        "url1 = 'https://raw.githubusercontent.com/TaillessTanuki/Berita_Banggai/main/ID-stopwords_banggai_addedwords.csv'\n",
        "df_stopword1 = pd.read_csv(url1, delimiter = \";\",encoding='cp1252')\n",
        "\n",
        "url2 = 'https://raw.githubusercontent.com/TaillessTanuki/Berita_Banggai/main/ID-Stopwords(MasDevid).csv'\n",
        "df_stopword2 = pd.read_csv(url2, delimiter = \";\",encoding='cp1252')\n",
        "\n",
        "custom_stopword1 = list(df_stopword1['stopword'])\n",
        "custom_stopword2 = list(df_stopword2['Stopwords'])\n",
        "\n",
        "factory = StopWordRemoverFactory()\n",
        "stopwords = factory.get_stop_words() + custom_stopword1 + custom_stopword2\n",
        "\n",
        "\n",
        "# 1st Stopwords Removal\n",
        "def filtering(text):\n",
        "  stop = stopword.remove(text)\n",
        "  return stop\n",
        "\n",
        "input_df['filtered'] = input_df['clean_double_ws'].apply(filtering)\n",
        "input_df['filtered']\n",
        "\n",
        "\n",
        "# Stemming\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "stem_factory = StemmerFactory()\n",
        "stemmer = stem_factory.create_stemmer()\n",
        "\n",
        "def stem(text):\n",
        "  return stemmer.stem(text)\n",
        "\n",
        "input_df['stemmed'] = input_df['filtered'].apply(stem)\n",
        "input_df['stemmed']\n",
        "\n",
        "\n",
        "# Tokenizing\n",
        "def tokenize(text):\n",
        "  tokens = nltk.tokenize.word_tokenize(text)\n",
        "  return tokens\n",
        "\n",
        "input_df['tokenized'] = input_df['stemmed'].apply(tokenize)\n",
        "input_df['tokenized']\n",
        "\n",
        "\n",
        "# 2nd Stopwords Removal\n",
        "# Define a function to remove stopwords from a list of tokens\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stopwords]\n",
        "\n",
        "# Apply the remove_stopwords function to each row in the 'tokenized' column\n",
        "input_df['filtered2'] = input_df['tokenized'].apply(lambda tokens: remove_stopwords(tokens))"
      ],
      "metadata": {
        "id": "06iX4I5RkRQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_df.head()"
      ],
      "metadata": {
        "id": "OUYdUqXHs83w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input_df.drop(columns=['predictions'], inplace=True)"
      ],
      "metadata": {
        "id": "ANhUfqmMzZtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokenized text to string representation\n",
        "#text_strings = [' '.join(doc) for doc in df['tokenized']]\n",
        "input_df['text2'] = input_df['tokenized'].apply(lambda x: ' '.join(x))\n"
      ],
      "metadata": {
        "id": "snJSk_5ULzDL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}